{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  `scatter` and `gather`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [`scatter_()`](https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_.html#torch.Tensor.scatter_)\n",
    "> Writes all values from the tensor `src` into `self` at the indices specified in the `index` tensor. \n",
    "\n",
    "For a 2-D tensor\n",
    "```\n",
    "self[index[i][j]][j] = src[i][j]  # if dim == 0\n",
    "self[i][index[i][j]] = src[i][j]  # if dim == 1\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  2,  3,  4,  5],\n",
       "        [ 6,  7,  8,  9, 10]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src = torch.arange(1, 11).reshape((2, 5))\n",
    "src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = torch.tensor([[0, 1, 2, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0, 4, 0],\n",
       "        [0, 2, 0, 0, 0],\n",
       "        [0, 0, 3, 0, 0]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(3, 5, dtype=src.dtype).scatter_(0, index, src)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `src[0][0]` (`index[0][0]` = 0) --> `self[index[0][0]][0]` == `self[0][0]` = 1\n",
    "- `src[0][1]` (`index[0][1]` = 1) --> `self[index[0][1]][1]` == `self[1][1]` = 2\n",
    "- `src[0][2]` (`index[0][2]` = 2) --> `self[index[0][2]][2]` == `self[2][2]` = 3\n",
    "- `src[0][3]` (`index[0][3]` = 0) --> `self[index[0][3]][3]` == `self[0][3]` = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4, 2, 3, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(3, 5, dtype=src.dtype).scatter_(1, index, src)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `src[0][0]` (`index[0][0]` = 0) --> `self[0][index[0][0]]` == `self[0][0]` = 1\n",
    "- `src[0][1]` (`index[0][1]` = 1) --> `self[0][index[0][1]]` == `self[0][1]` = 2\n",
    "- `src[0][2]` (`index[0][2]` = 2) --> `self[0][index[0][2]]` == `self[0][2]` = 3\n",
    "- `src[0][3]` (`index[0][3]` = 0) --> `self[0][index[0][3]]` == `self[0][0]` = 4 (**overlapped** with first row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.0000, 2.0000, 2.4600, 2.0000],\n",
       "        [2.0000, 2.0000, 2.0000, 2.4600]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.full((2, 4), 2.).scatter_(\n",
    "    1, torch.tensor([[2], [3]]),\n",
    "    1.23, reduce='multiply')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `src[0][0]` (`index[0][0]` = 2) --> `self[0][index[0][0]]` == `self[0][2]` * 1.23 = 2.46 \n",
    "- `src[1][0]` (`index[1][0]` = 3) --> `self[1][index[1][0]]` == `self[1][3]` * 1.23 = 2.46\n",
    "\n",
    "\n",
    "Similary for `reduce='add'`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One Hot Encoding using `scatter_()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1, 2, 4, 2, 1, 0, 3])\n",
    "num_classes = 5\n",
    "\n",
    "reshaped_x = x.view(-1, 1)\n",
    "one_hot = torch.full((x.shape[0], num_classes), 0).scatter_(\n",
    "    1, # dimension\n",
    "    reshaped_x, # index from reshaped x\n",
    "    1, # value to fill for one-hot encoding\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1],\n",
       "         [2],\n",
       "         [4],\n",
       "         [2],\n",
       "         [1],\n",
       "         [0],\n",
       "         [3]]),\n",
       " tensor([[0, 1, 0, 0, 0],\n",
       "         [0, 0, 1, 0, 0],\n",
       "         [0, 0, 0, 0, 1],\n",
       "         [0, 0, 1, 0, 0],\n",
       "         [0, 1, 0, 0, 0],\n",
       "         [1, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 1, 0]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reshaped_x, one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [`gather()`](https://pytorch.org/docs/stable/generated/torch.gather.html#torch.gather)\n",
    "> Gathers values along an axis specified by `dim`.\n",
    "\n",
    "\n",
    "For a 2-D tensor the output is specified by:\n",
    "\n",
    "```\n",
    "out[i][j] = input[index[i][j]][j]  # if dim == 0\n",
    "out[i][j] = input[i][index[i][j]]  # if dim == 1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1],\n",
       "        [4, 3]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.tensor([[1, 2, 5], [3, 4, 7]])\n",
    "torch.gather(t, 1, torch.tensor([[0, 0], [1, 0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `output[0][0]` (`index[0][0]` = 0) = `input[0][index[0][0]]` == `input[0][0]` = 1\n",
    "- `output[0][1]` (`index[0][1]` = 0) = `input[0][index[0][1]]` == `input[0][0]` = 1\n",
    "- `output[1][0]` (`index[1][0]` = 1) = `input[1][index[1][0]]` == `input[1][1]` = 4\n",
    "- `output[1][1]` (`index[1][1]` = 0) = `input[1][index[1][1]]` == `input[1][0]` = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate NLL (negative log likelihood) loss with `gather()` \n",
    "\n",
    "$$ H(p, q) = -\\sum_{K}\\log(p(x))q(x) $$\n",
    "\n",
    "where $p(x)$ is prediction and $q(x)$ is ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = torch.tensor([\n",
    "    [0.6, 0.5, 0.8, 0.3, 0.1],\n",
    "    [0.1, 0.9, 0.4, 0.2, 0.2],\n",
    "    [0.1, 0.3, 0.4, 0.2, 0.9],  \n",
    "]) # raw predictions of each sample (across all 5 classes)\n",
    "target_gt = torch.tensor([2, 1, 4]) # targets with class id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.4982, -1.5982, -1.2982, -1.7982, -1.9982],\n",
       "        [-1.9148, -1.1148, -1.6148, -1.8148, -1.8148],\n",
       "        [-1.9318, -1.7318, -1.6318, -1.8318, -1.1318]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# log softmax probabilities\n",
    "logprob = F.log_softmax(pred, dim=-1)\n",
    "logprob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.2982],\n",
       "        [1.1148],\n",
       "        [1.1318]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "- logprob.gather(\n",
    "    dim=1, # only gather data at axis=1 (since axis=0 refers to different data sample in the batch)\n",
    "    index=target_gt.unsqueeze(1), # gather value based on ground truth (only select index of true label) \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `enisum`\n",
    "\n",
    "https://stackoverflow.com/questions/55894693/understanding-pytorch-einsum\n",
    "\n",
    "- [`torch.enisum()`](https://pytorch.org/docs/stable/_modules/torch/functional.html#einsum)\n",
    "- [`numpy.enisum()`](https://numpy.org/devdocs/reference/generated/numpy.einsum.html)\n",
    "\n",
    "- NumPy allows both small case and capitalized letters `[a-zA-Z]` for the \"subscript string\" whereas PyTorch allows only the small case letters `[a-z]`.\n",
    "\n",
    "- NumPy accepts nd-arrays, plain Python lists (or tuples), list of lists (or tuple of tuples, list of tuples, tuple of lists) or even PyTorch tensors as operands (i.e. inputs). This is because the operands have only to be array_like and not strictly NumPy nd-arrays. On the contrary, PyTorch expects the operands (i.e. inputs) strictly to be PyTorch tensors. It will throw a `TypeError` if you pass either plain Python lists/tuples (or its combinations) or NumPy nd-arrays.\n",
    "\n",
    "- NumPy supports lot of keyword arguments (for e.g. `optimize`) in addition to `nd-arrays` while PyTorch doesn't offer such flexibility yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "aten = torch.tensor([\n",
    "    [11, 12, 13, 14],\n",
    "    [21, 22, 23, 24],\n",
    "    [31, 32, 33, 34],\n",
    "    [41, 42, 43, 44],\n",
    "])\n",
    "bten = torch.tensor([\n",
    "    [1, 1, 1, 1],\n",
    "    [2, 2, 2, 2],\n",
    "    [3, 3, 3, 3],\n",
    "    [4, 4, 4, 4],\n",
    "])\n",
    "\n",
    "vec = torch.tensor([0, 1, 2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[130, 130, 130, 130],\n",
       "        [230, 230, 230, 230],\n",
       "        [330, 330, 330, 330],\n",
       "        [430, 430, 430, 430]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum('ij, jk -> ik', aten, bten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract elements along the main-diagonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([11, 22, 33, 44])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum('ii -> i', aten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([11, 22, 33, 44])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.diag(aten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hadamard product (i.e. element-wise product of two tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 11,  12,  13,  14],\n",
       "        [ 42,  44,  46,  48],\n",
       "        [ 93,  96,  99, 102],\n",
       "        [164, 168, 172, 176]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum('ij, ij -> ij', aten, bten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Element-wise squaring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 121,  144,  169,  196],\n",
       "        [ 441,  484,  529,  576],\n",
       "        [ 961, 1024, 1089, 1156],\n",
       "        [1681, 1764, 1849, 1936]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum('ij, ij -> ij', aten, aten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 121,  144,  169,  196],\n",
       "        [ 441,  484,  529,  576],\n",
       "        [ 961, 1024, 1089, 1156],\n",
       "        [1681, 1764, 1849, 1936]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aten ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 4, 9])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum('i, i -> i', vec, vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 4, 9])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec * vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trace (i.e. sum of main-diagonal elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(110)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum('ii -> ', aten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[11, 21, 31, 41],\n",
       "        [12, 22, 32, 42],\n",
       "        [13, 23, 33, 43],\n",
       "        [14, 24, 34, 44]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum('ij -> ji', aten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[11, 21, 31, 41],\n",
       "        [12, 22, 32, 42],\n",
       "        [13, 23, 33, 43],\n",
       "        [14, 24, 34, 44]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aten.transpose(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outer Product (of vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0],\n",
       "        [0, 1, 2, 3],\n",
       "        [0, 2, 4, 6],\n",
       "        [0, 3, 6, 9]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum('i, j -> ij', vec, vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 4, 9])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec * vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inner Product (of vectors) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(14)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum('i, i -> ', vec, vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(14)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.dot(vec, vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sum along axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([104, 108, 112, 116])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum('ij -> j', aten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([104, 108, 112, 116])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(aten, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Matrix Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_tensor_1 = torch.arange(2 * 4 * 3).reshape(2, 4, 3)\n",
    "batch_tensor_2 = torch.arange(2 * 3 * 4).reshape(2, 3, 4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  20,   23,   26,   29],\n",
       "         [  56,   68,   80,   92],\n",
       "         [  92,  113,  134,  155],\n",
       "         [ 128,  158,  188,  218]],\n",
       "\n",
       "        [[ 632,  671,  710,  749],\n",
       "         [ 776,  824,  872,  920],\n",
       "         [ 920,  977, 1034, 1091],\n",
       "         [1064, 1130, 1196, 1262]]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.bmm(batch_tensor_1, batch_tensor_2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  20,   23,   26,   29],\n",
       "         [  56,   68,   80,   92],\n",
       "         [  92,  113,  134,  155],\n",
       "         [ 128,  158,  188,  218]],\n",
       "\n",
       "        [[ 632,  671,  710,  749],\n",
       "         [ 776,  824,  872,  920],\n",
       "         [ 920,  977, 1034, 1091],\n",
       "         [1064, 1130, 1196, 1262]]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum(\"bij, bjk -> bik\", batch_tensor_1, batch_tensor_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sum over multiple axes (i.e. marginalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5, 4, 6, 8, 2, 7, 9])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nDten = torch.randn((3,5,4,6,8,2,7,9))\n",
    "nDten.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-106.5269,   58.0306])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# marginalize out dimension 5 (i.e. \"n\" here)\n",
    "esum = torch.einsum(\"ijklmnop -> n\", nDten)\n",
    "esum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# marginalize out axis 5 (i.e. sum over rest of the axes)\n",
    "tsum = torch.sum(nDten, dim=(0, 1, 2, 3, 4, 6, 7))\n",
    "torch.allclose(tsum, esum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double Dot Products / Frobenius inner product (same as: torch.sum(hadamard-product) cf. 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1300)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum(\"ij, ij -> \", aten, bten)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "02cdac520ec7133de76ba57889f241b5b57a036339fda5892f85ea2dfaba9805"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('pytorch_workspace': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
